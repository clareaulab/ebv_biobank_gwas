{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HLA haplotype reconstruction using T1K\n",
    "\n",
    "Jacob Gutierrez\n",
    "\n",
    "This notebook runs T1K haplotyping on the All of Us workbench using `dsub`. The docker image can be accessed here: https://hub.docker.com/r/jacobog02/jg-t1k . The overview section contains general instructions on how the dsub command was executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: USER_NAME=jacobog02\n"
     ]
    }
   ],
   "source": [
    "USER_NAME = os.getenv('OWNER_EMAIL').split('@')[0].replace('.','-')\n",
    "\n",
    "# Save this Python variable as an environment variable to use within %%bash cells.\n",
    "%env USER_NAME={USER_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and define bash function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the tutorial: https://workbench.researchallofus.org/workspaces/aou-rw-6221d5ec/howtousedsubintheresearcherworkbenchv7/analysis/preview/1.%20dsub%20set%20up%20and%20read%20me.ipynb⁠  \n",
    "\n",
    "This shell function passes defaults for several dsub parameters so that the actual call is cleaner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional\n",
    "#!pip3 install --upgrade dsub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ~/aou_dsub.bash\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "function aou_dsub () {\n",
    "\n",
    "  # Get a shorter username to leave more characters for the job name.\n",
    "  local DSUB_USER_NAME=\"$(echo \"${OWNER_EMAIL}\" | cut -d@ -f1)\"\n",
    "\n",
    "  # For AoU projects, network name is \"network\".\n",
    "  local AOU_NETWORK=network\n",
    "  local AOU_SUBNETWORK=subnetwork\n",
    "\n",
    "  dsub \\\n",
    "      --provider google-cls-v2 \\\n",
    "      --user-project \"${GOOGLE_PROJECT}\"\\\n",
    "      --project \"${GOOGLE_PROJECT}\"\\\n",
    "      --image 'gcr.io/jg-public-docker-gcp/jg-t1k:latest' \\\n",
    "      --network \"${AOU_NETWORK}\" \\\n",
    "      --subnetwork \"${AOU_SUBNETWORK}\" \\\n",
    "      --service-account \"$(gcloud config get-value account)\" \\\n",
    "      --user \"${DSUB_USER_NAME}\" \\\n",
    "      --regions us-central1 \\\n",
    "      --logging \"${WORKSPACE_BUCKET}/dsub/logs/{job-name}/{user-id}/$(date +'%Y%m%d/%H%M%S')/{job-id}-{task-id}-{task-attempt}.log\" \\\n",
    "      \"$@\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to path \n",
    "%%bash\n",
    "\n",
    "echo source ~/aou_dsub.bash >> ~/.bashrc\n",
    "source ~/.bashrc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload hg38 reference sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain reference files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ref; samtools faidx Homo_sapiens_assembly38.fasta;\n",
    "!/home/jupyter/bin/gatk-4.2.6.0/gatk CreateSequenceDictionary -R Homo_sapiens_assembly38.fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `small_hla_regions_nounmapped.intervals` file is the output of `HLA_haplotype_construction/compute_hla_regions.R`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy these files to the google bucket file path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp ref/small_hla_regions_nounmapped.intervals ${WORKSPACE_BUCKET}/ref/small_hla_regions_nounmapped.intervals\n",
    "\n",
    "## DO NOT USE `gsutil cp` for big files... this would be done with a single core and is VERY slow. \n",
    "## `gcloud storage cp` does multithreaded upload, so the limits are by cores and networking speed. \n",
    "## i.e. gsutil cp ~ 1.8 MBPS | gcloud storage cp ~ 20-25MBPS -> 10 times faster via gcloud storage.\n",
    "!gcloud storage cp Homo_sapiens_assembly38.fasta ${WORKSPACE_BUCKET}/ref/Homo_sapiens_assembly38.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls ${WORKSPACE_BUCKET}/ref/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing scripts/yield_dsub_tasks_v5.R\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/yield_dsub_tasks_v5.R\n",
    "\n",
    "#!/bin/R\n",
    "\n",
    "library(data.table)\n",
    "library(dplyr)\n",
    "\n",
    "# All AoU IDs mapped to corresponding WGS cram files\n",
    "## This can be obtained via: gs://fc-aou-datasets-controlled/v8/wgs/cram/manifest.csv\n",
    "df <- fread(\"data/manifest.csv\")\n",
    "\n",
    "# Subset to individuals of EUR ancestry. See: EBV DNA Quantification / Format EBV DNA covariates notebook.\n",
    "ancestry_df <- fread(\"data/ancestry_preds.tsv\")\n",
    "EUR_IDs <- ancestry_df %>% filter(ancestry_pred == \"eur\") %>% pull(research_id)\n",
    "df <- df %>% filter(person_id %in% EUR_IDs)\n",
    "\n",
    "# Request all samples that exist in the output folder\n",
    "prev_run <-  system(\"gsutil ls ${WORKSPACE_BUCKET}/t1k/out/\", intern = T)\n",
    "prev_run <- prev_run %>% basename() %>% gsub(\"_genotype.tsv\",\"\",.)\n",
    "\n",
    "# Exclude previously run sample with genotype.tsv from new runs \n",
    "`%ni%` <- Negate(`%in%`)\n",
    "df <- df %>% filter(person_id %ni% prev_run) \n",
    "\n",
    "use_v <- df[,2] %>% pull()\n",
    "\n",
    "# Create output dir  \n",
    "out_p <- \"dsub_files_euro_f/\"\n",
    "dir.create(out_p,showWarnings=F)\n",
    "\n",
    "## Batches of ~90 should be roughly 12 hours of run time. This way we can get a sense of the time needed\n",
    "size <- 90\n",
    "\n",
    "split_l <- split(use_v, ceiling(seq_along(use_v)/size))\n",
    "names(split_l) <- paste0(\"Task_\", names(split_l))\n",
    "names(split_l) <- paste0(out_p,names(split_l), \"_input.txt\")\n",
    "\n",
    "# Write test output\n",
    "catch <- lapply(seq_along(split_l) , function(i) writeLines(split_l[[i]],names(split_l)[i]))\n",
    "\n",
    "# Create task file\n",
    "gproj <- Sys.getenv(\"WORKSPACE_BUCKET\")\n",
    "task_out <- paste0(out_p,\"taskfile.tsv\")\n",
    "header <- c(\"--input input_p\")\n",
    "out_df <- data.frame(V1 = paste0(gproj,\"/\", names(split_l)))\n",
    "writeLines(paste(header, collapse = \"\\t\"), task_out)\n",
    "fwrite(out_df, task_out, col.names = F, append = T )\n",
    "system(sprintf(\"gsutil -m cp -r %s/* ${WORKSPACE_BUCKET}/%s/\",out_p,out_p) , intern = T)\n",
    "\n",
    "## output is dsub_files_test2/taskfile.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!Rscript scripts/yield_dsub_tasks_v5.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dsub bash script "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bash script running T1K for all crams. `proc_t1k` processes one cram (individual), and the script loops through all EUR individuals using the `dsub_files_test2/taskfile.tsv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/t1k_singlerun_mnt.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/t1k_singlerun_mnt.sh\n",
    "#!/bin/bash\n",
    "\n",
    "#set -o errexit\n",
    "set -o nounset\n",
    "\n",
    "function proc_t1k (){\n",
    "\n",
    "## Input parameters: \n",
    "## 1) one_cram is a single cram input path \"gs::\"\"\n",
    "## 1) Regions.interval \"gs::\"\n",
    "## 2) hg38 fasta 'gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta'\n",
    "## 3) Google project is environment variable\n",
    "## 4) output path # in dsub call --output-recursive out_path=\"${WORKSPACE_BUCKET}/data/T1K/\"\n",
    "\n",
    "# Grab basename from cram\n",
    "one_cram=$1 ## as function\n",
    "basename=`basename \"${one_cram}\" .cram | cut -f2 -d \"_\"`\n",
    "prefix=\"${basename}_T1K\"\n",
    "mkdir -p ${prefix}\n",
    "    \n",
    "# Get reference filepaths \n",
    "regions=${ref_bucket}/small_hla_regions_nounmapped.intervals\n",
    "reference=${ref_bucket}/Homo_sapiens_assembly38.fasta\n",
    "\n",
    "\n",
    "# Get paired R1 R2 fastqs \n",
    "# Requires that the input fasta has both a .fai and .dict indexes in the same directory\n",
    "gatk PrintReads -I \"${one_cram}\" \\\n",
    "    -L \"${regions}\" -R \"${reference}\" \\\n",
    "    --gcs-project-for-requester-pays \"${the_proj}\" \\\n",
    "    --cloud-prefetch-buffer 0 --cloud-index-prefetch-buffer 0 -ip 1 \\\n",
    "    -O /dev/stdout | gatk SamToFastq I=/dev/stdin VALIDATION_STRINGENCY=SILENT \\\n",
    "    F=\"${prefix}/${basename}\"_hla_R1.fq F2=\"${prefix}/${basename}\"_hla_R2.fq \n",
    "\n",
    "\n",
    "# Run T1K \n",
    "/gatk/T1K/run-t1k -t 2 --noExtraction --preset hla-wgs -f /gatk/T1K/hlaidx/hlaidx_dna_seq.fa \\\n",
    "-1 \"${prefix}/${basename}\"_hla_R1.fq -2 \"${prefix}/${basename}\"_hla_R2.fq  -o \"${prefix}/${basename}\"\n",
    "\n",
    "\n",
    "# Export result to gsutil\n",
    "gsutil cp \"${prefix}/${basename}_genotype.tsv\" \"${out_path}/\"\n",
    "\n",
    "# Remove the input directory (not relevant for single sample, but will be for loops)\n",
    "rm -rf \"${prefix}/\"\n",
    "} \n",
    "\n",
    "run_v=(`cat ${input_p}`)\n",
    "\n",
    "for one_cram in ${run_v[@]};do\n",
    "\n",
    "# Start time\n",
    "echo \"start_time,\" ${one_cram} \",\" $(date +%s)\n",
    "\n",
    "proc_t1k ${one_cram}\n",
    "\n",
    "# End time\n",
    "echo \"end_time,\" ${one_cram} \",\" $(date +%s)\n",
    "\n",
    "    \n",
    "done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!gsutil cp scripts/t1k_singlerun_mnt.sh ${WORKSPACE_BUCKET}/scripts/t1k_singlerun_mnt.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOB_NAME=t1k-task-euro\n"
     ]
    }
   ],
   "source": [
    "# Use hyphens and not whitespace, since it will become part of the bucket path\n",
    "JOB_NAME='t1k-task-euro'\n",
    "\n",
    "# Save this Python variable as an environment variable \n",
    "%env JOB_NAME={JOB_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --out JOB_ID\n",
    "\n",
    "source ~/aou_dsub.bash # created above\n",
    "\n",
    "intask=\"dsub_files_euros/taskfile.tsv\"\n",
    "\n",
    "aou_dsub --name \"${JOB_NAME}\" --boot-disk-size 20 --machine-type n2-standard-2 --logging \"${WORKSPACE_BUCKET}/t1k/logging\" --env the_proj=\"${GOOGLE_PROJECT}\" --input-recursive ref_bucket=\"gs://fc-secure-0a5076ba-84f3-4336-8ae9-f629927dcc61/ref/\" --tasks $intask --output-recursive out_path=\"${WORKSPACE_BUCKET}/t1k/out/\" --script \"${WORKSPACE_BUCKET}/scripts/t1k_singlerun_mnt.sh\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOB_ID=t1k-task-v--jacobog02--250501-194508-00\n"
     ]
    }
   ],
   "source": [
    "# Save this Python variable \n",
    "%env JOB_ID={JOB_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check job status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!dstat --provider google-cls-v2 --project terra-vpc-sc-ae102581 --location us-central1 --jobs 't1k-task-e--jacobog02--250502-213635-98' --users 'jacobog02' --status '*' --format json | jq --arg key \"status-message\" '.[] | .[$key]' -  | sort | uniq -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "a_data=`dstat --provider google-cls-v2 --project terra-vpc-sc-ae102581 --location us-central1 --jobs 't1k-task-e--jacobog02--250502-213635-98' --users 'jacobog02' --status '*'`\n",
    "\n",
    "$a_data | wc -l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# --full\n",
    "dstat \\\n",
    "    --provider google-cls-v2 \\\n",
    "    --project \"${GOOGLE_PROJECT}\" \\\n",
    "    --location us-central1 \\\n",
    "    --jobs \"${JOB_ID}\" \\\n",
    "    --users \"${USER_NAME}\" \\\n",
    "    --status '*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!gsutil ls ${WORKSPACE_BUCKET}/t1k/logging/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls -lh ${WORKSPACE_BUCKET}/t1k/out/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy over outputs to local folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p t1k_out/\n",
    "!gcloud storage cp -r ${WORKSPACE_BUCKET}/t1k/out/ t1k_out/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
